---
title: "Twitter Analysis of Lyft"
author: "Luying Cao"
date: "2017/12/15"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r, warning=FALSE}
library(twitteR)
library(rjson)
library(bit64)
library(httr)
library(RgoogleMaps)
library(ggmap)
library(ggplot2)
library(maptools)
library(sp)
library(streamR)
library(ROAuth)
library(splitstackshape)
library(stringr)
library(tidyr)
library(tidyverse)
library(reshape2)
library(ggplot2)
library(dplyr)
library(tidytext)
library(RColorBrewer)
library(wordcloud)
```

```{r}
api_key <- 	"0Tj23PB9z0IcZJnCOt9bnqWqy"
api_secret <- "ObmBp5sV9Bk0e4Qa2aC2zj3fUEvFZa5TaTpdBN5qeLIncAXiSZ"
access_token <- "1713606835-b24BGGCml2q6Wx8Fb88LptHXUkg1EAN8FxM9ket"
access_token_secret <- "Gx6c6zZInFV1zFkE0NMui3mmNYKjat4WRu93YgprvJR2b"

setup_twitter_oauth(api_key, 
                    api_secret, 
                    access_token, 
                    access_token_secret)

requestURL <- "https://api.twitter.com/oauth/request_token"
accessURL <- "https://api.twitter.com/oauth/access_token"
authURL <- "https://api.twitter.com/oauth/authorize"
consumerKey <- 	"LFNRqX5i1PkB69SjEEncXWloq"
consumerSecret <- "4sDHqY6aLm7PRfJLxpq6GsWqphZxzX3dXLjssSLXYhO8wPwL3F"
my_oauth <- OAuthFactory$new(consumerKey = consumerKey, consumerSecret = consumerSecret, 
                             requestURL = requestURL, accessURL = accessURL, authURL = authURL)
my_oauth$handshake(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))
save(my_oauth, file = "my_oauth.Rdata")

load("my_oauth.Rdata")
```

For this Twitter data analysis project, I choose the keyword and brand "Lyft" to study. Lyft is a transportation network company to provide car transportations through mobile app. It was founded in 2012, and is now grow rapidly and becoming the largest competitor of Uber.

To start with, I used the "searchTwitter" function to generate 1,000 tweets that contain the keyword "#Lyft" in it. Second, I cleaned the texts in order to make sure that there is no special character or symbol in them.

```{r}
lyft <- searchTwitter("#Lyft", n = 1000, lang = 'en')
lyftdf <- twListToDF(lyft)

#clean data
cleantweets <- function(lyftdf) {
  lyftdf <- text %>%
    gsub("<.*>", "", .) %>% # remove emojis
    gsub("&amp;", "", .) %>% # remove &
    gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", .) %>% # remove retweet entities
    gsub("[[:punct:]]", "", .) %>% # remove punctuation
    gsub("http\\w+", "", .) %>% # remove html links
    iconv(from = "latin1", to = "ASCII", sub="") %>% # remove emoji and bizarre signs
    gsub("^\\s+|\\s+$", "", .) %>% # remove unnecessary spaces
    tolower
  return(lyftdf)
}
```


After the text cleaning, I was ready to do some analyses. The first one I am going to find out is the word frequency in those texts. I used R to generate a word frequency table, which also identifies the nature of the words. From the table, it can be found out that the most frequent positive word "free" appears seven times of the most frequent negative word is "vex". It can be then inferred that customers using Lyft generally receive a cheap or even free ride and they are pretty satisfied. In addition, I did the sentiment analysis to have a better view of the general picture of both positive and negative words. According to the sentiment analysis, there are way more positive words than negative words, which can be concluded that the majority of customers who use Lyft have a generally satisfied experience.


```{r}
#Sentiment Analysis for Lyft
lyftdf1 <- lyftdf[, -c(2,3,4,5,6,7,8,9,10,11,12,13,14,15,16)]
lyftdf1 <- data.frame(lyftdf1)
lyftdf1$id <- c(1:nrow(lyftdf1))
lyftdf1 <- data.frame(lapply(lyftdf1, as.character), stringsAsFactors=FALSE)

lyftdf1_text <- melt(data = lyftdf1, id.vars = "id")
lyftdf1_text <- data_frame(line = lyftdf1_text$id, text = lyftdf1_text$value)
lyftdf1_text <- lyftdf1_text %>%
  unnest_tokens(word, text)

data(stop_words)

lyftdf1_text <- lyftdf1_text %>%
  anti_join(stop_words) 

lyftdf1_text %>%
  dplyr::count(word, sort = TRUE) 

nrcjoy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

lyftdf1_text_sentiment_stat <- lyftdf1_text %>%
  inner_join(nrcjoy) %>%
  dplyr::count(word, sort = TRUE)

lyftdf1_text$line <- as.numeric(lyftdf1_text$line)

bing_word_counts <- lyftdf1_text %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

sentiment_analysis <- bing_word_counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()

sentiment_analysis

write.csv(lyftdf1, file = "lyft_data.csv")
```


Then, I generated two word clouds to have a more direct sight on the texts. In word clouds, words that appear more often will be shown bigger and bolder. Again, it can be easily seen that Lyft customers feel happy about their experiences.


```{r}
#Wordclouds for Lyft
wordcloud_withoutsentiment <- lyftdf1_text %>%
  anti_join(stop_words) %>%
  dplyr::count(word) %>%
  with(wordcloud(word, n, max.words = 100))

wordcloud_withoutsentiment

wordcloud_withsentiment <- lyftdf1_text %>%
  inner_join(get_sentiments("bing")) %>%
  dplyr::count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100,scale = c(2,.5),title.size=1.5)

wordcloud_withsentiment
```


Last but not least, I generated a map of United States showing where the tweets were published. Each location is represented by a red point in the map. From the map, it can be found out that there are more tweets published either in New England area or California, which can be inferred that Lyft are more popular in these places. At the same time, only a few or none tweets have been published in the Middle and Northern part, which can be inferred that Lyft is not that popular or maybe do not exist in those places.

```{r}
#Create ggmap of lat and lon

##NYmap
yo <- searchTwitteR("#Lyft", n = 2000)
yodf <- twListToDF(yo)
loc <- -1*is.na(yodf$longitude) + 1
sum(loc)
loc1  <- which(loc==1)
locations <- data.frame(yodf$latitude[loc1], yodf$longitude[loc1])
locations$yodf.latitude.loc1. <- as.numeric(locations$yodf.latitude.loc1.)
locations$yodf.longitude.loc1. <- as.numeric(locations$yodf.longitude.loc1.)

nymap_data <- ggmap(get_map(location = "new york"), scale = 2, zoom = 8, source = "google", maptype = "roadmap") +
                geom_point(data = locations, aes(x = yodf.longitude.loc1., y = yodf.latitude.loc1.), 
                           alpha = 0.5, size = 2, color = "orange")
  

##USmap
filterStream("USmap.json", 
             track=c("Lyft"), 
             locations = c(-125, 25, -66, 50), 
             timeout=200, oauth=my_oauth)
netmap<-parseTweets("USmap.json", verbose = TRUE)
ck1 <- sum(netmap$lat>0, na.rm = TRUE)
ck2 <- sum(netmap$place_lat>0, na.rm = TRUE)
ck3 <- sum(!is.na(netmap$location))
map.data <- map_data("state")   
netpoints <- data.frame(x = as.numeric(netmap$lon),  
                       y = as.numeric(netmap$lat))
netpoints <- netpoints[netpoints$y > 25, ]  
netpoints<-filter(netpoints,y>19&y<65,x>(-161.7)&x<(-68.01))
USmap_data <- ggplot(map.data) + 
  geom_map(aes(map_id = region),  
           map = map.data,  
           fill = "light grey",             
           color = "grey20", size = 0.25) + 
  expand_limits(x = map.data$long, y = map.data$lat) +            
  theme(axis.line = element_blank(),  
        axis.text = element_blank(),  
        axis.ticks = element_blank(),                     
        axis.title = element_blank(),  
        panel.background = element_blank(),  
        panel.border = element_blank(),                     
        panel.grid.major = element_blank(), 
        plot.background = element_blank(),                     
        plot.margin = unit(0 * c( -1.5, -1.5, -1.5, -1.5), "lines")) +  
        geom_point(data = netpoints,             
        aes(x = x, y = y), size = 1,  
        alpha = 1/5, color = "red")
USmap_data
```

In conclusion, customers who use Lyft generally have a satisfied experience. It is essential for Lyft to keep its advantage of customer satisfaction. At the same time, there are some limitations of this data analysis project and there are a few things that I would like to address in the future. First, I could gather more tweets about Lyft to do more analyses. Second, I could gather tweets contain keyword "#Uber", which is Lyft's largest competitor, to generate comparisons. Third, I could find out which state or city has the highest amount of tweets or highest level of satisfaction of customers using Lyft. 







